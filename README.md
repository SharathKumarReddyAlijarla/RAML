# RAML
## Investigating the Impact of Architectural Variations on the Performance of Siamese Neural Networks

Siamese Neural Networks (SNNs) have gained popularity due to their ability to
compare and identify similarity between input data points. In this study, we investigate
the impact of architecture details on the performance of Siamese Neural
Networks. We explore various parameters that may differ in SNNs, such as the
choice of neural network architectures (e.g., CNNs, MLPs), number of layers, neurons
per layer, size of the output layer (i.e., final embedding), and hyperparameters
(e.g., learning rate, weight initialization scheme). To support our findings, we also
look into relevant literature. The results of this study will provide valuable insights
into designing efficient and effective Siamese Neural Networks for similarity-based
tasks.
